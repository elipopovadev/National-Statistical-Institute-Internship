{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e252540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time as time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "43b48dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalation\n",
    "s = Service((r'C:\\Users\\eli\\Desktop\\AllMyRepos\\National-Statistical-Institute\\Job Posts Statistics\\Scraping Jobs.bg and Zaplata.bg/chromedriver.exe'))\n",
    "chromeOptions = Options()\n",
    "chromeOptions.headless = False\n",
    "driver = webdriver.Chrome(service=s, options=chromeOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e19f0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists\n",
    "job_titles_ = []\n",
    "informations_ = []\n",
    "dates_ = []\n",
    "companies_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "526259b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(1,5):\n",
    "    # URL for current city\n",
    "    url='https://www.jobs.bg/front_job_search.php?subm=1&location_sid=' + str(k) + '&distance=3'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Cookies\n",
    "    if k == 1:\n",
    "        cookies_button = driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div/div[2]/button')\n",
    "        cookies_button.click()\n",
    "        time.sleep(2)\n",
    "        driver.execute_script('window.scrollTo(0,200)')  \n",
    "        time.sleep(2)\n",
    "        yes_no_button = driver.find_element(By.XPATH, '/html/body/div[3]/div[7]/div/div[1]/div[2]/div[2]/button[1]/div')\n",
    "        yes_no_button.click()\n",
    "\n",
    "    # Category_button\n",
    "    category_button = driver.find_element(By.ID, 'categoriesChip')\n",
    "    category_button.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    for i in range(2, 59):\n",
    "        # all categories\n",
    "        all_categories = driver.find_elements(By.XPATH,'//label[contains(@for,\\'categories\\')]')    \n",
    "        try:\n",
    "            current_category = all_categories[i]\n",
    "            driver.execute_script(\"arguments[0].click();\",current_category)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            continue \n",
    "\n",
    "        confirm_button = driver.find_element(By.CLASS_NAME, 'mdc-fab__label')\n",
    "        driver.execute_script(\"arguments[0].click();\", confirm_button)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Scrolling\n",
    "        last_height = driver.execute_script('return document.documentElement.scrollTop')\n",
    "        while True:\n",
    "            # Soup Object\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            dates = soup.findAll('div', class_ = 'card-date')\n",
    "            for date in dates:\n",
    "                try:\n",
    "                    dates_.append(date.text.split()[0])\n",
    "                except:\n",
    "                    dates_.append('n/a')\n",
    "\n",
    "            titles = soup.findAll('div', class_ = 'card-title')\n",
    "            for title in titles:\n",
    "                try:\n",
    "                    job_titles_.append(title.text.replace('star','').strip())\n",
    "                except:\n",
    "                    job_titles_.append('n/a')\n",
    "\n",
    "            informations = soup.findAll('div', class_ = 'card-info')\n",
    "            for information in informations:\n",
    "                try:\n",
    "                    informations_.append(information.text)\n",
    "                except:\n",
    "                    informations_.append('n/a')\n",
    "\n",
    "            companies = soup.findAll('div', class_ = 'secondary-text')\n",
    "            for company in companies:\n",
    "                try:\n",
    "                    companies_.append(company.text) \n",
    "                except:\n",
    "                    companies_.append('n/a')\n",
    "\n",
    "            driver.execute_script(\"window.scrollBy(0, 3000);\") \n",
    "            time.sleep(2)\n",
    "            new_height = driver.execute_script('return document.documentElement.scrollTop;')\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        # Category_button at the beginning \n",
    "        category_button = driver.find_element(By.ID, 'categoriesChip')\n",
    "        driver.execute_script(\"arguments[0].click();\", category_button)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Clean button\n",
    "        clean_button = driver.find_element(By.XPATH, '//*[@id=\"clearCategoriesLink\"]')\n",
    "        time.sleep(2)\n",
    "        clean_button.click()\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4952b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(5,307):\n",
    "    if l == 25:\n",
    "        continue\n",
    "    else:\n",
    "        # URL for current Town\n",
    "        url='https://www.jobs.bg/front_job_search.php?subm=1&location_sid=' + str(l) \n",
    "        driver.get(url)\n",
    "\n",
    "        # Scrolling\n",
    "        last_height = driver.execute_script('return document.documentElement.scrollTop')\n",
    "        while True:\n",
    "            # Soup Object\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            dates = soup.findAll('div', class_ = 'card-date')\n",
    "            for date in dates:\n",
    "                try:\n",
    "                    dates_.append(date.text.split()[0])\n",
    "                except:\n",
    "                    dates_.append('n/a')\n",
    "\n",
    "            titles = soup.findAll('div', class_ = 'card-title')\n",
    "            for title in titles:\n",
    "                try:\n",
    "                    job_titles_.append(title.text.replace('star','').strip())\n",
    "                except:\n",
    "                    job_titles_.append('n/a')\n",
    "\n",
    "            informations = soup.findAll('div', class_ = 'card-info')\n",
    "            for information in informations:\n",
    "                try:\n",
    "                    informations_.append(information.text)\n",
    "                except:\n",
    "                    informations_.append('n/a')\n",
    "\n",
    "            companies = soup.findAll('div', class_ = 'secondary-text')\n",
    "            for company in companies:\n",
    "                try:\n",
    "                    companies_.append(company.text) \n",
    "                except:\n",
    "                    companies_.append('n/a')\n",
    "\n",
    "            driver.execute_script(\"window.scrollBy(0, 3000);\") \n",
    "            time.sleep(2)\n",
    "            new_height = driver.execute_script('return document.documentElement.scrollTop;')\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "299f483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for Work from home\n",
    "url='https://www.jobs.bg/front_job_search.php?subm=1&is_home_based=1'\n",
    "driver.get(url)\n",
    "\n",
    "# Scrolling\n",
    "last_height = driver.execute_script('return document.documentElement.scrollTop')\n",
    "while True:\n",
    "    # Soup Object\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    dates = soup.findAll('div', class_ = 'card-date')\n",
    "    for date in dates:\n",
    "        try:\n",
    "            dates_.append(date.text.split()[0])\n",
    "        except:\n",
    "            dates_.append('n/a')\n",
    "\n",
    "    titles = soup.findAll('div', class_ = 'card-title')\n",
    "    for title in titles:\n",
    "        try:\n",
    "            job_titles_.append(title.text.replace('star','').strip())\n",
    "        except:\n",
    "            job_titles_.append('n/a')\n",
    "\n",
    "    informations = soup.findAll('div', class_ = 'card-info')\n",
    "    for information in informations:\n",
    "        try:\n",
    "            informations_.append(information.text)\n",
    "        except:\n",
    "            informations_.append('n/a')\n",
    "\n",
    "    companies = soup.findAll('div', class_ = 'secondary-text')\n",
    "    for company in companies:\n",
    "        try:\n",
    "            companies_.append(company.text) \n",
    "        except:\n",
    "            companies_.append('n/a')\n",
    "\n",
    "    driver.execute_script(\"window.scrollBy(0, 3000);\") \n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script('return document.documentElement.scrollTop;')\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d874a54",
   "metadata": {},
   "source": [
    "### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "26d0451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.DataFrame({ 'Date' : dates_,'Job_Title': job_titles_,'Information': informations_, \n",
    "                                                            'Company' : companies_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16a738da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244023 entries, 0 to 244022\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   Date         244023 non-null  object\n",
      " 1   Job_Title    244023 non-null  object\n",
      " 2   Information  244023 non-null  object\n",
      " 3   Company      244023 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 7.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4924f17",
   "metadata": {},
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2c88a7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 32579 entries, 0 to 243920\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Date         32579 non-null  object\n",
      " 1   Job_Title    32579 non-null  object\n",
      " 2   Information  32579 non-null  object\n",
      " 3   Company      32579 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_jobs.drop_duplicates(keep='first', inplace=True, ignore_index=False)\n",
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84318859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cef20121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.to_excel('ads_JOBS.BG.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
